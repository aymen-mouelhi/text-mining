{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\"\n",
    "\n",
    "doc2 = \"\"\" Python is a very nice programming programming programming language\n",
    "language language language used by many researchers, engineers and data scientists.\"\"\"\n",
    "\n",
    "doc3 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\"\n",
    "\n",
    "\n",
    "collection = [doc1, doc2, doc3]\n",
    "collection =[x.split(' ') for x in collection]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Python', 'is', 'a', '2000', 'made-for-TV', 'horror', 'movie', 'directed', 'by', 'Richard\\nClabaugh.', 'The', 'film', 'features', 'several', 'cult', 'favorite', 'actors,', 'including', 'William\\nZabka', 'of', 'The', 'Karate', 'Kid', 'fame,', 'Wil', 'Wheaton,', 'Casper', 'Van', 'Dien,', 'Jenny', 'McCarthy,\\nKeith', 'Coogan,', 'Robert', 'Englund', '(best', 'known', 'for', 'his', 'role', 'as', 'Freddy', 'Krueger', 'in', 'the\\nA', 'Nightmare', 'on', 'Elm', 'Street', 'series', 'of', 'films),', 'Dana', 'Barron,', 'David', 'Bowe,', 'and', 'Sean\\nWhalen.', 'The', 'film', 'concerns', 'a', 'genetically', 'engineered', 'snake,', 'a', 'python,', 'that\\nescapes', 'and', 'unleashes', 'itself', 'on', 'a', 'small', 'town.', 'It', 'includes', 'the', 'classic', 'final\\ngirl', 'scenario', 'evident', 'in', 'films', 'like', 'Friday', 'the', '13th.', 'It', 'was', 'filmed', 'in', 'Los', 'Angeles,\\n', 'California', 'and', 'Malibu,', 'California.', 'Python', 'was', 'followed', 'by', 'two', 'sequels:', 'Python\\n', 'II', '(2002)', 'and', 'Boa', 'vs.', 'Python', '(2004),', 'both', 'also', 'made-for-TV', 'films.'], ['', 'Python', 'is', 'a', 'very', 'nice', 'programming', 'programming', 'programming', 'language\\nlanguage', 'language', 'language', 'used', 'by', 'many', 'researchers,', 'engineers', 'and', 'data', 'scientists.'], ['The', 'Colt', 'Python', 'is', 'a', '.357', 'Magnum', 'caliber', 'revolver', 'formerly\\nmanufactured', 'by', \"Colt's\", 'Manufacturing', 'Company', 'of', 'Hartford,', 'Connecticut.\\nIt', 'is', 'sometimes', 'referred', 'to', 'as', 'a', '\"Combat', 'Magnum\".[1]', 'It', 'was', 'first', 'introduced\\nin', '1955,', 'the', 'same', 'year', 'as', 'Smith', '&amp;', \"Wesson's\", 'M29', '.44', 'Magnum.', 'The', 'now', 'discontinued\\nColt', 'Python', 'targeted', 'the', 'premium', 'revolver', 'market', 'segment.', 'Some', 'firearm\\ncollectors', 'and', 'writers', 'such', 'as', 'Jeff', 'Cooper,', 'Ian', 'V.', 'Hogg,', 'Chuck', 'Hawks,', 'Leroy\\nThompson,', 'Renee', 'Smeets', 'and', 'Martin', 'Dougherty', 'have', 'described', 'the', 'Python', 'as', 'the\\nfinest', 'production', 'revolver', 'ever', 'made.']]\n"
     ]
    }
   ],
   "source": [
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf(word, doc):\n",
    "    # Get the frequency of a word in a document\n",
    "    countW = doc.count(word)\n",
    "    return countW / float(len(doc))\n",
    "\n",
    "def idf(word, collection):\n",
    "    countW_C = sum([1 for doc in collection if word in doc])\n",
    "    idf_ = len(collection)/float(countW_C+1)\n",
    "    return math.log(idf_)\n",
    "\n",
    "def tfidf(word, doc, collection):\n",
    "    return tf(word, doc)*idf(word, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'that', 'once', 'them', 'a', 'those', 'doing', \"hasn't\", 'most', 'with', 'against', 'yourself', 'him', 'be', 'how', 'mightn', 'ma', 'shan', 'couldn', 'yourselves', 'was', \"needn't\", 'are', 'their', 'd', \"doesn't\", \"should've\", \"you'd\", 'shouldn', \"haven't\", 'not', \"she's\", 'aren', 'mustn', 'if', 'has', 'he', \"aren't\", 'hasn', 'having', \"wasn't\", 'above', \"couldn't\", 'an', 'o', 'herself', 'about', 'very', 'ain', 're', 'weren', 'had', 'then', 'from', 'no', \"won't\", 'same', 'been', 'didn', 'hadn', \"didn't\", 'while', 'only', 'who', 'at', \"you've\", 'for', 'there', 'why', 'all', \"shouldn't\", 'won', 'did', 'i', 'you', 'too', \"don't\", 'yours', 'when', 'do', 'few', 'what', 'so', 'or', 'nor', 'y', \"it's\", 'which', 'during', 'to', 'but', 'between', 'himself', 'doesn', 'own', \"mustn't\", 'than', 'don', 'wasn', 'in', 'here', 'his', 'until', 'more', 'whom', 'm', \"mightn't\", 'and', 'by', 'down', 'her', 'up', 'were', 'on', 'me', 'am', 'off', 's', \"wouldn't\", 'because', \"hadn't\", 'any', \"that'll\", 'as', 'it', 'under', 'theirs', 'should', 'before', 'below', 'is', 'can', 'just', 'isn', 'after', 'we', 'your', 'of', \"weren't\", 'over', 'other', 'my', 'its', 'the', 'they', 'needn', 'both', 'll', \"you're\", 'each', 'she', 'through', 'itself', 'some', 've', 'into', 'hers', \"shan't\", 't', 'our', 'out', 'now', 'myself', 'where', 'does', 'such', 'further', 'will', 'being', 'wouldn', 'these', 'this', 'ourselves', 'again', 'have', \"isn't\", 'ours', 'haven', \"you'll\", 'themselves'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 0.01057735064629994), ('on', 0.007051567097533294), ('film', 0.007051567097533294), ('made-for-TV', 0.007051567097533294)]\n",
      "------------------\n",
      "[('programming', 0.060819766216224654), ('language', 0.04054651081081644), ('researchers,', 0.02027325540540822), ('language\\nlanguage', 0.02027325540540822)]\n",
      "------------------\n",
      "[('revolver', 0.01539740916866447), ('V.', 0.005132469722888157), ('Connecticut.\\nIt', 0.005132469722888157), ('Hogg,', 0.005132469722888157)]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# print most 4 top important words per doc\n",
    "for i,doc in enumerate(collection):\n",
    "    important=[]\n",
    "    for word in set(doc):\n",
    "        \n",
    "            important.append((word,tfidf(word,doc,collection)))\n",
    "    important=sorted(important,key=lambda x:x[1])[::-1]\n",
    "    print(important[0:4])\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stems of second document\n",
      "\t\n",
      "used\tuse\n",
      "many\tmani\n",
      "programming\tprogram\n",
      "data\tdata\n",
      "scientists.\tscientists.\n",
      "Python\tpython\n",
      "very\tveri\n",
      "a\ta\n",
      "engineers\tengin\n",
      "is\tis\n",
      "nice\tnice\n",
      "language\n",
      "language\tlanguage\n",
      "languag\n",
      "language\tlanguag\n",
      "researchers,\tresearchers,\n",
      "and\tand\n",
      "by\tby\n"
     ]
    }
   ],
   "source": [
    "# NLTK is a library fro natural language processing related tasks\n",
    "# it has various stemmers\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print(\"Getting stems of second document\")\n",
    "for word in set(collection[1]):\n",
    "    print(word  + \"\\t\" + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Euler is the father of graph theory\",\n",
    "             \"Graph theory studies the properties of graphs\",\n",
    "             \"Bioinformatics studies the application of efficient algorithms in biological problems\",\n",
    "             \"DNA sequences are very complex biological structures\",\n",
    "             \"Genes are parts of a DNA sequence\",\n",
    "             \"This is Euler bioinformatics sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf-idf vectorizer does automaic tf-idf calculation on a list of strings (documents)\n",
    "# it maintains the list of terms found\n",
    "# we can also find the idf values of specific words\n",
    "# compute similarity among documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also use tf-idf to retrieve documents with queries based on the similarity of the query to all docs\n",
    "documents2 = [\"Euler is the father of graph theory\",\n",
    "             \"Graph theory studies the properties of graphs\",\n",
    "             \"Bioinformatics studies the application of efficient algorithms in biological problems\",\n",
    "             \"DNA sequences are very complex structures\",\n",
    "             \"Genes are parts of a DNA sequence\",\n",
    "             \"Run to the hills, run for your lives\",\n",
    "             \"The lonenliness of the long distance runner\",\n",
    "             \" Heaven can wait til another day\",\n",
    "             \"Road runner and coyote is my favorite cartoon\",\n",
    "             ]\n",
    "              #\"Heaven can wait\" # this is our query\n",
    "query=\" heaven can wait\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'euler': 12, 'father': 13, 'graph': 16, 'theory': 33, 'studies': 32, 'properties': 25, 'graphs': 17, 'bioinformatics': 3, 'application': 2, 'efficient': 11, 'algorithms': 0, 'biological': 4, 'problems': 24, 'dna': 10, 'sequences': 30, 'complex': 6, 'structures': 31, 'genes': 15, 'parts': 23, 'sequence': 29, 'run': 27, 'hills': 19, 'lives': 20, 'lonenliness': 21, 'long': 22, 'distance': 9, 'runner': 28, 'heaven': 18, 'wait': 35, 'til': 34, 'another': 1, 'day': 8, 'road': 26, 'coyote': 7, 'favorite': 14, 'cartoon': 5}\n"
     ]
    }
   ],
   "source": [
    "model = TfidfVectorizer(ngram_range=(1,1), stop_words=english_stopwords)\n",
    "# fit model\n",
    "model.fit(documents2)\n",
    "\n",
    "print(model.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.60943791  2.60943791  2.60943791  2.60943791  2.60943791  2.60943791\n",
      "  2.60943791  2.60943791  2.60943791  2.60943791  2.2039728   2.60943791\n",
      "  2.60943791  2.60943791  2.60943791  2.60943791  2.2039728   2.60943791\n",
      "  2.60943791  2.60943791  2.60943791  2.60943791  2.60943791  2.60943791\n",
      "  2.60943791  2.60943791  2.60943791  2.60943791  2.2039728   2.60943791\n",
      "  2.60943791  2.60943791  2.2039728   2.2039728   2.60943791  2.60943791]\n"
     ]
    }
   ],
   "source": [
    "# Get IDF\n",
    "print(model.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.60943791243\n"
     ]
    }
   ],
   "source": [
    "# Get idf for a word\n",
    "print(model.idf_[model.vocabulary_['euler']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 33)\t0.456265802382\n",
      "  (0, 16)\t0.456265802382\n",
      "  (0, 13)\t0.540205069929\n",
      "  (0, 12)\t0.540205069929\n",
      "  (1, 33)\t0.415099653442\n",
      "  (1, 32)\t0.415099653442\n",
      "  (1, 25)\t0.491465580248\n",
      "  (1, 17)\t0.491465580248\n",
      "  (1, 16)\t0.415099653442\n",
      "  (2, 32)\t0.325978363988\n",
      "  (2, 24)\t0.38594863782\n",
      "  (2, 11)\t0.38594863782\n",
      "  (2, 4)\t0.38594863782\n",
      "  (2, 3)\t0.38594863782\n",
      "  (2, 2)\t0.38594863782\n",
      "  (2, 0)\t0.38594863782\n",
      "  (3, 31)\t0.518938071798\n",
      "  (3, 30)\t0.518938071798\n",
      "  (3, 10)\t0.43830335718\n",
      "  (3, 6)\t0.518938071798\n",
      "  (4, 29)\t0.518938071798\n",
      "  (4, 23)\t0.518938071798\n",
      "  (4, 15)\t0.518938071798\n",
      "  (4, 10)\t0.43830335718\n",
      "  (5, 27)\t0.816496580928\n",
      "  (5, 20)\t0.408248290464\n",
      "  (5, 19)\t0.408248290464\n",
      "  (6, 28)\t0.43830335718\n",
      "  (6, 22)\t0.518938071798\n",
      "  (6, 21)\t0.518938071798\n",
      "  (6, 9)\t0.518938071798\n",
      "  (7, 35)\t0.4472135955\n",
      "  (7, 34)\t0.4472135955\n",
      "  (7, 18)\t0.4472135955\n",
      "  (7, 8)\t0.4472135955\n",
      "  (7, 1)\t0.4472135955\n",
      "  (8, 28)\t0.38903906952\n",
      "  (8, 26)\t0.460610627968\n",
      "  (8, 14)\t0.460610627968\n",
      "  (8, 7)\t0.460610627968\n",
      "  (8, 5)\t0.460610627968\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.54020507\n",
      "   0.54020507  0.          0.          0.4562658   0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.4562658   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create the matrix tfidf\n",
    "tfidfM = model.transform(documents2)\n",
    "print(tfidfM) # sparce non zero data\n",
    "print(tfidfM.todense()[0,:]) # full matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 35)\t0.707106781187\n",
      "  (0, 18)\t0.707106781187\n"
     ]
    }
   ],
   "source": [
    "# Create vector for the query\n",
    "qv = model.transform([query])\n",
    "print(qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.632455532034\n"
     ]
    }
   ],
   "source": [
    "# Get Inner product of qv and documents to get how similar they are\n",
    "inner_product = qv.dot(tfidfM.T)\n",
    "print(inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Euler is the father of graph theory\",\n",
    "             \"Graph theory studies the properties of graphs\",\n",
    "             \"Graph theory is cool!\",\n",
    "             \"DNA sequences are very complex biological structures\",\n",
    "             \"Genes are biological structures that are parts of a DNA sequence\",\n",
    "             \"Genes are very important biological structures\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Latent semantic analysis: project document to the clustetr (concepts) they belong to\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=english_stopwords)\n",
    "# fit model\n",
    "tfidf_vectorizer.fit(documents)\n",
    "# Transform\n",
    "tfidfM = tfidf_vectorizer.transform(documents)\n",
    "\n",
    "U,S,V = np.linalg.svd(tfidfM.toarray())\n",
    "\n",
    "proj = tfidfM.toarray().dot(V[:2,:].T)\n",
    "\n",
    "# we keep 2 eigen vecors from the term to term similarity part (V) \n",
    "# which will represent two clsters/topics in our collection\n",
    "# after, we project the data on two these concepts (eigenvectors) to replace words with their projections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x119c73cc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoAkNcoAkKRG/R/hamYCb1vBtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119854940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot the projected documents -we know that there are two clusters by looking at the documents\n",
    "# and we see the same in the new projection\n",
    "\n",
    "# Clearly we have 2 topics !\n",
    "plt.scatter(proj[:,0], proj[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TF-IDF as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets use the previous in a classification task\n",
    "# load data\n",
    "data=pd.read_csv('movie_reviews.csv',header=None)\n",
    "data.columns=['text','label']\n",
    "# Part 2: data preprocessing\n",
    "############\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and predict with  BernouliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
